{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOkcMD-vAzdF"
      },
      "source": [
        "# Setup "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KHFeH2W7S7Q",
        "outputId": "2b91b6a2-3b84-4126-c995-2791f9ca05f0"
      },
      "outputs": [],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_5V8U337u05",
        "outputId": "0f9a21eb-2485-4ead-97a4-db8d7b076fbc"
      },
      "outputs": [],
      "source": [
        "# !pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "AAa2EK5S7OVf",
        "outputId": "149ffdf3-32a1-49a5-9071-7bc1fa154619"
      },
      "outputs": [],
      "source": [
        "from openai import AzureOpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "url_base = 'https://ubc-cv-sherlock.s3.us-west-2.amazonaws.com/oops_val_v1_frames/'\n",
        "subsets = ['preevent', 'postevent', 'event']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6j7VL6HCA2eK"
      },
      "source": [
        "## OpenAI calls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "o8Mqd6khuo8i",
        "outputId": "a097a706-53bc-44a4-affb-777ad52f720c"
      },
      "outputs": [],
      "source": [
        "from keys import *\n",
        "GPT4V_KEY=GPT4_KEY\n",
        "\n",
        "\n",
        "client = AzureOpenAI(\n",
        "      api_version=\"2024-02-15-preview\",\n",
        "      azure_endpoint=\"https://culturalsnapsweden.openai.azure.com/\",\n",
        "      api_key=GPT4V_KEY)\n",
        "\n",
        "\n",
        "headers = {\n",
        "    \"Content-Type\": \"application/json\",\n",
        "    \"api-key\": GPT4V_KEY,\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_response(messages):\n",
        "  response = client.chat.completions.create(\n",
        "      # engine=\"gpt4v\", # engine = \"deployment_n\n",
        "      model=\"gpt4\",\n",
        "      messages= messages,\n",
        "    #   response_format=\"json\",\n",
        "      max_tokens=500,\n",
        "  )\n",
        "  return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(ground_truth_texts, model_texts):\n",
        "    ###  ground truth\n",
        "    gold = \"\\n\".join([f\"{i+1}. {item['Explanation']}\" for i, item in enumerate(ground_truth_texts)])\n",
        "    #predicted = \"\\n\".join([f\"{i+1}. {item['Explanation']}\" for i, item in enumerate(model_texts)])\n",
        "    predicted = f\"1. {model_texts[0]['Explanation']}\"\n",
        "\n",
        "\n",
        "    task1_prompt = f\"\"\"You will be given given 3 ground truth (gt) descriptions of a video (set1) and a single descriptions to evaluate (set2).\n",
        "\n",
        "Your task is to compare set2 against set1 and rate it based on two metrics:\n",
        "\n",
        "1. Specificity: Measures the extent to which the set2 description includes detailed and distinguishing features present in set1/ground truth description.\n",
        "\n",
        "2. Similarity: Measures how closely the content of the set2 description matches the content of the set1/ground truth description. This metric evaluates the overlap in information, phrasing, and key elements between the two descriptions.\n",
        "\n",
        "Both metrics need to be scored on a scale of 1-5. \n",
        "\n",
        "Return a python dictionary, containing two keys, Specificity and Similarity. The value of each key is a python tuple containing the score out of 5 and the reason for the score. \n",
        "\n",
        "Here are the ground truth (set1) descriptions: \n",
        "{gold}\n",
        "Here is the evaluation (set2) description:\n",
        "{predicted}\n",
        "\"\"\"\n",
        "    system_prompt = \"You are an expert in evaluating text generated from AI models.\"\n",
        "    messages = []\n",
        "    messages.append({\"role\": \"system\", \"content\": [\n",
        "            {\n",
        "                \"type\": \"text\",\n",
        "                \"text\": system_prompt\n",
        "            },\n",
        "            ],})\n",
        "\n",
        "    messages.append({\"role\": \"user\", \"content\": [\n",
        "\n",
        "            {\n",
        "                \"type\": \"text\",\n",
        "                \"text\":  task1_prompt\n",
        "            },\n",
        "            ],})\n",
        "\n",
        "    res1 = get_response(messages)\n",
        "    messages.append({'role': res1.choices[0].message.role, 'content': res1.choices[0].message.content})\n",
        "\n",
        "    return messages, res1.choices[0].message.content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "main_prompt = \"\"\"You are a masterful English language grader. Your expertise is used to evaluate video descriptions against a set of reference descriptions. Your goal is to evaluate the video description based on the following:\n",
        "\n",
        "1. Semantic similarity: measures how closely the meaning of the video description aligns with the reference descriptions. It focuses on the overall context and key actions/events described.\n",
        "To evaluate this, you will rate each of the following from 1-5, 1 being low and 5 being high:\n",
        "(a) Action/Event Matching: Identify and compare the main actions or events described.\n",
        "(b) Context and Outcome: Assess if the context and outcomes described in the video description are similar to those in the reference descriptions.\n",
        "(c) Use of Similar Words/Phrases: Note the use of similar or synonymous words and phrases.\n",
        "\n",
        "2. Level of detail: measures how detailed and specific the video description is compared to the reference descriptions. It focuses on the richness of the description and the number of distinct details provided, and not the meaning of the content itself.\n",
        "To evaluate this, you will rate each of the following from 1-5:\n",
        "(a) Detail Level: Count the number of specific details and descriptive elements. If the number of details in the video description match the number of details in the reference prompts, then give it a three. If more, give it a 4 or 5, and if lower, give it a 1 or 2 rating.\n",
        "(b) Clarity of Actions/Events: Evaluate how clearly and specifically actions/events are described. If the clarity matches that of the reference descriptions, then rate it a 3. If clarity is higher, you can rate it 4 or 5, and if lower, rate it 1 or 2.\n",
        "\n",
        "Grade the video description, and explain your evaluation. Your output should be in JSON, following this template:\n",
        "{\"Action/Event Matching\" : {\"Rating\": X, \"Explanation\": \"...\"}...\n",
        "\"\"\"\n",
        "\n",
        "def evaluate(ground_truth_texts, model_texts):\n",
        "    ###  ground truth\n",
        "    gold = \"\\n\".join([f\"{i+1}. {item['Explanation']}\" for i, item in enumerate(ground_truth_texts)])\n",
        "    #predicted = \"\\n\".join([f\"{i+1}. {item['Explanation']}\" for i, item in enumerate(model_texts)])\n",
        "    results = []\n",
        "    for i in range(0, 3):\n",
        "        predicted = f\"{model_texts[i]['Explanation']}\"\n",
        "\n",
        "        task1_prompt = f\"\"\"Here's a video description:\n",
        "        {predicted}\n",
        "\n",
        "        Here's a reference set of descriptions:\n",
        "        {gold}\"\"\"\n",
        "\n",
        "        system_prompt = main_prompt\n",
        "        messages = []\n",
        "        messages.append({\"role\": \"system\", \"content\": [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": system_prompt\n",
        "                },\n",
        "                ],})\n",
        "\n",
        "        messages.append({\"role\": \"user\", \"content\": [\n",
        "\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\":  task1_prompt\n",
        "                },\n",
        "                ],})\n",
        "\n",
        "        res1 = get_response(messages)\n",
        "        results.append(res1.choices[0].message.content)\n",
        "        messages.append({'role': res1.choices[0].message.role, 'content': res1.choices[0].message.content})\n",
        "    \n",
        "    results = '['+','.join(results)+']'\n",
        "\n",
        "    return messages, results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "main_prompt = \"\"\"You are a masterful English language grader. Your expertise is used to evaluate video descriptions against a set of reference descriptions. Your goal is to evaluate the video description based on the following:\n",
        "\n",
        "2. Level of detail: measures how detailed and specific the video description is compared to the reference descriptions. It focuses on the richness of the description and the number of distinct details provided, and not the meaning of the content itself.\n",
        "To evaluate this, you will rate each of the following from 1-5:\n",
        "(a) Detail Level: Count the number of specific details and descriptive elements. If the number of details in the video description match the number of details in the reference prompts, then give it a three. If more, give it a 4 or 5, and if lower, give it a 1 or 2 rating.\n",
        "(b) Clarity of Actions/Events: Evaluate how clearly and specifically actions/events are described. If the clarity matches that of the reference descriptions, then rate it a 3. If clarity is higher, you can rate it 4 or 5, and if lower, rate it 1 or 2.\n",
        "\n",
        "Grade the video description, and explain your evaluation. Your output should be in JSON, following this template:\n",
        "{\"Action/Event Matching\" : {\"Rating\": X, \"Explanation\": \"...\"}...\n",
        "\"\"\"\n",
        "\n",
        "def evaluate(ground_truth_texts, model_texts):\n",
        "    ###  ground truth\n",
        "    gold = \"\\n\".join([f\"{i+1}. {item['Explanation']}\" for i, item in enumerate(ground_truth_texts)])\n",
        "    #predicted = \"\\n\".join([f\"{i+1}. {item['Explanation']}\" for i, item in enumerate(model_texts)])\n",
        "    results = []\n",
        "    for i in range(0, 3):\n",
        "        predicted = f\"{model_texts[i]['Explanation']}\"\n",
        "\n",
        "        task1_prompt = f\"\"\"Here's a video description:\n",
        "        {predicted}\n",
        "\n",
        "        Here's a reference set of descriptions:\n",
        "        {gold}\"\"\"\n",
        "\n",
        "        system_prompt = main_prompt\n",
        "        messages = []\n",
        "        messages.append({\"role\": \"system\", \"content\": [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": system_prompt\n",
        "                },\n",
        "                ],})\n",
        "\n",
        "        messages.append({\"role\": \"user\", \"content\": [\n",
        "\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\":  task1_prompt\n",
        "                },\n",
        "                ],})\n",
        "\n",
        "        res1 = get_response(messages)\n",
        "        results.append(res1.choices[0].message.content)\n",
        "        messages.append({'role': res1.choices[0].message.role, 'content': res1.choices[0].message.content})\n",
        "    \n",
        "    results = '['+','.join(results)+']'\n",
        "\n",
        "    return messages, results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import AzureOpenAI\n",
        "#original data\n",
        "import json\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inference on gpt4 - text only\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def extract_overall_json(json_string):\n",
        "    \"\"\"\n",
        "    Extracts JSON from the given string and processes it.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if pd.isna(json_string):\n",
        "            return None\n",
        "        json_string = json_string.replace(\"json\", \"\").replace(\"```\", \"\").strip()\n",
        "        json_string = json.loads(json_string)\n",
        "        return json_string\n",
        "    except (json.JSONDecodeError, TypeError, KeyError, IndexError):\n",
        "        return json_string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "json_file = \"oops_val_v3_mf_2.json\"\n",
        "with open(json_file, 'r') as f:\n",
        "    data = json.load(f)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from tqdm import tqdm\n",
        "def process_json_file(data):\n",
        "\n",
        "    final_responses = []\n",
        "\n",
        "    # Open the JSON file in append mode\n",
        "    for item in tqdm(data[:5]):\n",
        "        task1_responses = item['task1_responses']\n",
        "        # print(task1_responses)\n",
        "        if task1_responses and len(task1_responses) > 1:\n",
        "            # image_urls = [url_base + f'{item[sub][0:-4]}/frame_{i}.jpg' for sub in subsets for i in range(1, 5)]\n",
        "        \n",
        "            all_messages, task1_responses = evaluate(item['task1_gt'], item['task1_responses'])\n",
        "            # print(task1_responses)\n",
        "            fr = item\n",
        "            # Add responses to the new item\n",
        "            fr['metric'] = task1_responses\n",
        "\n",
        "            final_responses.append(fr)\n",
        "\n",
        "    return final_responses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_data = process_json_file(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for item in new_data:\n",
        "    item['metric'] = extract_overall_json(item['metric'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Write the new data to a new JSON file\n",
        "with open(json_file.replace(\".json\", \"_metric.json\"), 'w') as f:\n",
        "    json.dump(new_data, f, indent=4,  separators=(',', ':'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import Video, HTML\n",
        "def video_display(idx):\n",
        "    video_link = f\"https://ubc-cv-sherlock.s3.us-west-2.amazonaws.com/oops/oops_val_v3/{idx}\"\n",
        "\n",
        "    # Embed the video using HTML\n",
        "    video_html = f\"<video controls src='{video_link}' width='500' height='400'>\"\n",
        "    display(HTML(video_html))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pprint\n",
        "for item in new_data:\n",
        "    video_display(item['preevent'])\n",
        "    print(\"HUMAN:\")\n",
        "    pprint.pp(item['task1_gt'])\n",
        "    print(\"AI:\")\n",
        "    pprint.pp(item['task1_responses'][0]['Explanation'])\n",
        "    pprint.pp(item['metric'])\n",
        "    print(\"#########################\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pprint\n",
        "for item in new_data:\n",
        "    video_display(item['preevent'])\n",
        "    print(\"HUMAN:\")\n",
        "    pprint.pp(item['task1_gt'])\n",
        "    print(\"AI:\")\n",
        "    pprint.pp(item['task1_responses'])\n",
        "    pprint.pp(item['metric'])\n",
        "    print(\"#########################\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfCntl-E7OVg"
      },
      "source": [
        "# OpenAI Assistant API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Doesn't work well..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'AzureOpenAI' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeys\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      4\u001b[0m GPT4V_KEY\u001b[38;5;241m=\u001b[39mGPT4_KEY\n\u001b[0;32m----> 7\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[43mAzureOpenAI\u001b[49m(\n\u001b[1;32m      8\u001b[0m       api_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2024-05-01-preview\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m       azure_endpoint\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://culturalsnapsweden.openai.azure.com/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m       api_key\u001b[38;5;241m=\u001b[39mGPT4V_KEY)\n\u001b[1;32m     13\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi-key\u001b[39m\u001b[38;5;124m\"\u001b[39m: GPT4V_KEY,\n\u001b[1;32m     16\u001b[0m }\n\u001b[1;32m     18\u001b[0m assistant \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mbeta\u001b[38;5;241m.\u001b[39massistants\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     19\u001b[0m     instructions\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mYou are a masterful English language grader. Your expertise is used to evaluate video descriptions against a set of reference descriptions. Your goal is to evaluate the video description based on the following:\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124m            1. Semantic similarity: measures how closely the meaning of the video description aligns with the reference descriptions. It focuses on the overall context and key actions/events described.\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     response_format\u001b[38;5;241m=\u001b[39m{ \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson_object\u001b[39m\u001b[38;5;124m\"\u001b[39m }\n\u001b[1;32m     37\u001b[0m )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'AzureOpenAI' is not defined"
          ]
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "from keys import *\n",
        "GPT4V_KEY=GPT4_KEY\n",
        "\n",
        "\n",
        "client = AzureOpenAI(\n",
        "      api_version=\"2024-05-01-preview\",\n",
        "      azure_endpoint=\"https://culturalsnapsweden.openai.azure.com/\",\n",
        "      api_key=GPT4V_KEY)\n",
        "\n",
        "\n",
        "headers = {\n",
        "    \"Content-Type\": \"application/json\",\n",
        "    \"api-key\": GPT4V_KEY,\n",
        "}\n",
        "\n",
        "assistant = client.beta.assistants.create(\n",
        "    instructions=\"\"\"You are a masterful English language grader. Your expertise is used to evaluate video descriptions against a set of reference descriptions. Your goal is to evaluate the video description based on the following:\n",
        "            1. Semantic similarity: measures how closely the meaning of the video description aligns with the reference descriptions. It focuses on the overall context and key actions/events described.\n",
        "            To evaluate this, you will rate each of the following from 1-5:\n",
        "            (a) Action/Event Matching: Identify and compare the main actions or events described.\n",
        "            (b) Context and Outcome: Assess if the context and outcomes described in the video description are similar to those in the reference descriptions.\n",
        "            (c) Use of Similar Words/Phrases: Note the use of similar or synonymous words and phrases.\n",
        "\n",
        "            2. Level of detail: measures how detailed and specific the video description is compared to the reference descriptions. It focuses on the richness of the description and the number of distinct details provided, not the content itself..\n",
        "            To evaluate this, you will rate each of the following from 1-5:\n",
        "            (a) Detail Level: Count the number of specific details and descriptive elements.\n",
        "            (b) Clarity of Actions/Events: Evaluate how clearly and specifically actions/events are described.\n",
        "            \n",
        "            When asked to grade, your output should always be in JSON, following this template:\n",
        "            {\"Action/Event Matching\" : {\"Rating\": X, \"Explanation\": \"...\"}...\n",
        "            \"\"\",\n",
        "    name=\"English Language Grader\",\n",
        "    model=\"gpt4\",\n",
        "    response_format={ \"type\": \"json_object\" }\n",
        ")\n",
        "print(assistant)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_assistant(ground_truth_texts, model_text):\n",
        "  reference = \"\\n\".join([f\"{i+1}. {item['Explanation']}\" for i, item in enumerate(ground_truth_texts)])\n",
        "\n",
        "  thread = client.beta.threads.create()\n",
        "  message = client.beta.threads.messages.create(\n",
        "    thread_id=thread.id,\n",
        "    role=\"user\",\n",
        "    content=f\"\"\"Here's a video description:\n",
        "    {model_text}\n",
        "\n",
        "    Here's a reference set of descriptions:\n",
        "    {reference}\n",
        "\n",
        "    Grade it and respond in JSON only.\"\"\")\n",
        "\n",
        "  run = client.beta.threads.runs.create_and_poll(\n",
        "    thread_id=thread.id,\n",
        "    assistant_id=assistant.id\n",
        "  )\n",
        "\n",
        "  #print(client.beta.threads.messages.data)\n",
        "  if run.status == 'completed': \n",
        "    messages = client.beta.threads.messages.list(\n",
        "      thread_id=thread.id\n",
        "    )\n",
        "    #print(messages._get_page_items())\n",
        "    return messages._get_page_items()[0].content\n",
        "  else:\n",
        "    print(run.status)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import json\n",
        "def process_json_file(data):\n",
        "\n",
        "    final_responses = []\n",
        "\n",
        "    # Open the JSON file in append mode\n",
        "    for item in tqdm(data[:5]):\n",
        "        task1_responses = item['task1_responses']\n",
        "        # print(task1_responses)\n",
        "        if task1_responses and len(task1_responses) > 1:\n",
        "            # image_urls = [url_base + f'{item[sub][0:-4]}/frame_{i}.jpg' for sub in subsets for i in range(1, 5)]\n",
        "        \n",
        "            task1_responses = evaluate_assistant(item['task1_gt'], item['task1_responses'][0]['Explanation'])\n",
        "            #print(task1_responses)\n",
        "            fr = item\n",
        "            # Add responses to the new item\n",
        "            fr['metric'] = json.loads(task1_responses[0].text.value)\n",
        "\n",
        "            final_responses.append(fr)\n",
        "\n",
        "    return final_responses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "process_json_file(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "# Define the endpoint and API version\n",
        "endpoint = \"https://culturalsnapsweden.openai.azure.com\"\n",
        "api_version = \"2024-05-01\"\n",
        "api_key = GPT4V_KEY\n",
        "\n",
        "# Construct the URL\n",
        "url = f\"{endpoint}/openai/models/gpt-4?api-version={api_version}\"\n",
        "\n",
        "# Define the headers\n",
        "headers = {\n",
        "    \"api-key\": api_key\n",
        "}\n",
        "\n",
        "# Make the GET request with headers\n",
        "response = requests.get(url, headers=headers)\n",
        "print(response._content)\n",
        "\n",
        "# Check if the request was successful\n",
        "if response.status_code == 200:\n",
        "    # Parse the JSON response\n",
        "    models = response.json()\n",
        "    print(models)\n",
        "else:\n",
        "    print(f\"Failed to retrieve models: {response.status_code}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "J5k4Ej-JA6rZ"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
